import cv2
import mediapipe as mp
import av
import time
import queue
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration

# ---------------- 1. í˜ì´ì§€ ì„¤ì • ë° ì„¸ì…˜ ì´ˆê¸°í™” ----------------
st.set_page_config(page_title="ìë™ ì œìŠ¤ì²˜ ì¹´ë©”ë¼", layout="centered")

# ì°ì€ ì‚¬ì§„ì„ ì €ì¥í•  ê³µê°„ (ìƒˆë¡œê³ ì¹¨ ë˜ì–´ë„ ìœ ì§€ë¨)
if "captured_image" not in st.session_state:
    st.session_state["captured_image"] = None

st.title("âœŒï¸ ì œìŠ¤ì²˜ ìë™ ìº¡ì²˜")
st.write("ì¹´ë©”ë¼ë¥¼ ì¼œê³  **V ì‚¬ì¸**ì„ í•´ë³´ì„¸ìš”. ìë™ìœ¼ë¡œ ì°íˆê³  ì‚¬ì§„ì´ ëœ¹ë‹ˆë‹¤.")

# STUN ì„œë²„ (ê²€ì€ í™”ë©´ ë°©ì§€)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# Mediapipe ì´ˆê¸°í™”
mp_face = mp.solutions.face_detection
mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils

# ---------------- 2. ë°±ê·¸ë¼ìš´ë“œ ì˜ìƒ ì²˜ë¦¬ê¸° ----------------
class VictoryProcessor(VideoTransformerBase):
    def __init__(self):
        self.face_detector = mp_face.FaceDetection(min_detection_confidence=0.6)
        self.hand_detector = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.6)
        self.result_queue = queue.Queue() # ë©”ì¸ í™”ë©´ìœ¼ë¡œ ì‚¬ì§„ ë³´ë‚´ëŠ” í†µë¡œ
        self.last_capture_time = 0
        self.captured = False # ì¤‘ë³µ ì´¬ì˜ ë°©ì§€ í”Œë˜ê·¸

    def is_victory(self, lms, w, h):
        def c(i):
            lm = lms.landmark[i]
            return int(lm.x * w), int(lm.y * h)
        try:
            # ê²€ì§€(8), ì¤‘ì§€(12) í´ì§ (Yì¢Œí‘œê°€ ë‚®ìŒ) / ì•½ì§€, ìƒˆë¼ ì ‘í˜ (Yì¢Œí‘œê°€ ë†’ìŒ)
            if (lms.landmark[8].y < lms.landmark[5].y and 
                lms.landmark[12].y < lms.landmark[9].y and 
                lms.landmark[16].y > lms.landmark[13].y and 
                lms.landmark[20].y > lms.landmark[17].y):
                return True
        except:
            pass
        return False

    def recv(self, frame):
        # ì´ë¯¸ ìº¡ì²˜í–ˆìœ¼ë©´ ì²˜ë¦¬ ì¤‘ë‹¨ (UI ì—…ë°ì´íŠ¸ ëŒ€ê¸°)
        if self.captured:
            return av.VideoFrame.from_ndarray(frame.to_ndarray(format="bgr24"), format="bgr24")

        img = frame.to_ndarray(format="bgr24")
        img_out = img.copy()
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w, _ = img.shape

        # ì–¼êµ´ & ì† ì¸ì‹
        face_res = self.face_detector.process(img_rgb)
        hand_res = self.hand_detector.process(img_rgb)
        
        face_detected = face_res.detections is not None
        victory_detected = False

        if hand_res.multi_hand_landmarks:
            for handLms in hand_res.multi_hand_landmarks:
                mp_draw.draw_landmarks(img_out, handLms, mp_hands.HAND_CONNECTIONS)
                if self.is_victory(handLms, w, h):
                    victory_detected = True

        # â˜… ì¡°ê±´ ë§Œì¡± ì‹œ ìë™ ìº¡ì²˜ â˜…
        if face_detected and victory_detected:
            current_time = time.time()
            if current_time - self.last_capture_time > 2.0: # ì¿¨íƒ€ì„ 2ì´ˆ
                self.last_capture_time = current_time
                self.captured = True # í”Œë˜ê·¸ ì„¸ì›€
                
                # ìº¡ì²˜ íš¨ê³¼ í…ìŠ¤íŠ¸
                cv2.putText(img_out, "CAPTURED!", (50, 100), 
                           cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 255), 3)
                
                # í(Queue)ì— ì›ë³¸ ì‚¬ì§„ ì „ì†¡
                self.result_queue.put(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

        return av.VideoFrame.from_ndarray(img_out, format="bgr24")

# ---------------- 3. ë©”ì¸ í™”ë©´ UI ë¡œì§ ----------------

# WebRTC ìŠ¤íŠ¸ë¦¬ë¨¸ ì‹¤í–‰
ctx = webrtc_streamer(
    key="gesture-cam",
    video_processor_factory=VictoryProcessor,
    rtc_configuration=RTC_CONFIGURATION,
    media_stream_constraints={"video": True, "audio": False}
)

# â˜… ìë™ ê°ì§€ ë£¨í”„ â˜…
# ì¹´ë©”ë¼ê°€ ì¼œì ¸ ìˆëŠ” ë™ì•ˆ, ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‚¬ì§„ì´ ë„˜ì–´ì˜¤ëŠ”ì§€ ê³„ì† ê°ì‹œí•©ë‹ˆë‹¤.
if ctx.state.playing:
    placeholder = st.empty() # ìƒíƒœ ë©”ì‹œì§€ í‘œì‹œìš©
    while True:
        if ctx.video_processor:
            try:
                # íì—ì„œ ì‚¬ì§„ì´ ë“¤ì–´ì™”ëŠ”ì§€ í™•ì¸ (ëŒ€ê¸° ì‹œê°„ 0.1ì´ˆ)
                result_img = ctx.video_processor.result_queue.get(timeout=0.1)
            except queue.Empty:
                result_img = None
            
            # ì‚¬ì§„ì´ ë„ì°©í–ˆë‹¤ë©´?
            if result_img is not None:
                # ì„¸ì…˜ì— ì €ì¥í•˜ê³  í™”ë©´ ìƒˆë¡œê³ ì¹¨ (Rerun)
                st.session_state["captured_image"] = result_img
                ctx.video_processor.captured = False # ìº¡ì²˜ í”Œë˜ê·¸ ì´ˆê¸°í™”
                st.rerun() # â˜… ì—¬ê¸°ì„œ ìë™ìœ¼ë¡œ í™”ë©´ì´ ê°±ì‹ ë©ë‹ˆë‹¤!
                break
        
        time.sleep(0.05) # CPU ê³¼ë¶€í•˜ ë°©ì§€

# ---------------- 4. ìº¡ì²˜ëœ ì‚¬ì§„ ë° ì €ì¥ ë²„íŠ¼ í‘œì‹œ ----------------
# í™”ë©´ì´ ìƒˆë¡œê³ ì¹¨ë˜ë©´ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„
st.markdown("---")
if st.session_state["captured_image"] is not None:
    st.success("ğŸ“¸ ìº¡ì²˜ ì™„ë£Œ! ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì €ì¥í•˜ì„¸ìš”.")
    
    # 1. ì‚¬ì§„ ë³´ì—¬ì£¼ê¸°
    st.image(st.session_state["captured_image"], caption="ë°©ê¸ˆ ì°ì€ ì‚¬ì§„", use_column_width=True)
    
    # 2. ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ìƒì„± (ì´ë¯¸ì§€ -> íŒŒì¼ ë³€í™˜)
    img_bgr = cv2.cvtColor(st.session_state["captured_image"], cv2.COLOR_RGB2BGR)
    ret, buffer = cv2.imencode(".jpg", img_bgr)
    if ret:
        st.download_button(
            label="â¬‡ï¸ ì‚¬ì§„ ì €ì¥í•˜ê¸° (Click to Save)",
            data=buffer.tobytes(),
            file_name=f"capture_{int(time.time())}.jpg",
            mime="image/jpeg"
        )
